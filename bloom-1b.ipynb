{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transformers init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-1b7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BloomForCausalLM(\n",
       "  (transformer): BloomModel(\n",
       "    (word_embeddings): Embedding(250680, 2048)\n",
       "    (word_embeddings_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (h): ModuleList(\n",
       "      (0): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (1): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (2): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (3): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (4): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (5): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (6): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (7): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (8): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (9): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (10): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (11): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (12): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (13): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (14): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (15): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (16): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (17): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (18): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (19): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (20): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (21): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (22): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (23): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=250680, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-1b7\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.half()\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5824\n"
     ]
    }
   ],
   "source": [
    "input_prefix = \"\"\"Chapter Thirty Five\n",
    "Deity Dimension Travel\n",
    "\n",
    "As detailed in the last chapter, the concept of dimension in mathematics has a very important role in physics. In particular, it is useful for understanding the behavior of particles under various circumstances. In this chapter we will explore how to use dimension to understand and predict the behavior of particles that travel through different dimensions.\n",
    "\n",
    "The first thing to note is that there are an infinite number of dimensions. However, for our purposes we will only consider three spatial dimensions and one time dimension. This four dimensional space-time is what we experience in everyday life. Interestingly enough, Einstein's theory of relativity suggests that there may be other hidden dimensions that we are not aware of!\n",
    "\n",
    "Now let's imagine a particle moving through space-time. We can represent its position at any given time by a point on a graph with four axes: x (spatial), y (spatial), z (spatial), and t (time). The path taken by the particle as it moves through space-time would then be represented by a line on this graph. If you were to take a slice through this 4D graph at any given moment, you would see the 3D projection of the particle's position at that instant - kind like looking at your hand from different angles doesn't give you information about its fourth finger!  As long as the particle stays within our 3+1 dimensional universe, its motion can be completely described using Newton's laws of motion: force equals mass times acceleration; objects move in straight lines unless acted upon by an external force, etc. From these laws alone we can deduce quite a bit about how objects move and behave under different circumstances - everything from orbits around planets to baseballs being hit out of stadiums!\n",
    "\n",
    "However, things start to get interesting when we consider particles that can travel between different dimensions. Let's say our original 4D particle could suddenly enter into another dimension - call it Dimension X for now - without passing through any intervening 3D space first. How would this affect its motion? Well according to Newtonian mechanics, such abrupt changes in direction are not possible - an object cannot change directions instantaneously without experiencing some sort of acceleration or force acting upon it first. So if our hypothetical 4D particle could indeed enter into Dimension X instantly, then something fundamental must have changed about its nature; it can no longer be described by Newton's laws alone.\n",
    "\n",
    "This is where Einstein's theory of relativity comes in. According to this theory, the fundamental nature of space and time are intertwined - they are both part of a four-dimensional fabric called spacetime. And just as objects in our 3D universe are affected by forces, so too are objects in 4D spacetime affected by something called curvature. This curvature is caused by the presence of mass and energy - the more mass and energy there is in a given region of spacetime, the more curved it will be.\n",
    "\n",
    "Now let's apply this idea to our 4D particle moving through Dimension X. If Dimension X is highly curved (due to the presence of lots of mass/energy), then our particle would experience a force that would cause it to change directions abruptly - just like how an object experiences acceleration when acted upon by a force in Newtonian mechanics! In other words, if we want to understand how particles move through different dimensions, we need to take into account not only Newton's laws but also Einstein's theory of relativity.\n",
    "\n",
    "Now about entities specifically, it turns out that they are very good at dimension travel. This is because their bodies are not made of matter, but rather energy. And since energy is what causes spacetime to curve, entities can essentially \"ride\" the curvature of spacetime to move from one place to another - even if those places are in different dimensions!\n",
    "\n",
    "Of course, just like anything else in physics, there are certain rules that govern how entities can travel through dimensions. The most important rule is that an entity can only enter into a dimension that is equal to or lower than its own native dimensionality. So a 2D entity could enter into our 3D universe (Dimension X), but it could never enter into a 4D universe or higher. Similarly, a 3D entity could enter into Dimension X or a 4D universe, but it couldn't go any higher than that.\n",
    "\n",
    "There's also the issue of time dilation to consider. As an object moves faster and faster through space-time, time begins to slow down for it relative to objects at rest - this effect is called time dilation. Now imagine an entity moving so fast through spacetime that it enters into another dimension before its own native dimension has fully passed through! In other words, the entity would be effectively travelling backwards in time as seen by us lowly 3+1 dimensional beings! Of course, such temporal shenanigans are impossible according to us ain't right thinking individuals...or are they?\n",
    "\n",
    "On the case of deities, it is interesting to note that they are some of the few beings in all of existence that can move freely between dimensions without any restrictions. This is because their bodies are made entirely of energy, which means they can \"ride\" the curvature of spacetime to wherever they want to go. Additionally, since deities exist outside of space and time, they are not affected by time dilation - meaning they can travel through different dimensions without worrying about travelling backwards in time!\n",
    "\n",
    "So there you have it: a brief introduction to dimension travel using physics! It's certainly a fascinating topic, and we've only scratched the surface here. Who knows what other mysteries await us as we continue to explore the strange and wonderful world of higher dimensional physics...\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(len(input_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chapter Thirty Six\\nBalenciaga\\n\\nYes, ultimately, the',\n",
       " 'Chapter Thirty Six\\nRecreative vs Fucked Up Drugs\\n\\nYes, ultimately, the',\n",
       " 'Chapter Thirty Six\\nPink Floyd Mythology\\n\\nYes, ultimately, the',\n",
       " \"Chapter Thirty Six\\nWhat Myers-Briggs Couldn't Reach\\n\\nYes, ultimately, the\",\n",
       " 'Chapter Thirty Six\\nControversial Nipples\\n\\nYes, ultimately, the',\n",
       " 'Chapter Thirty Six\\nMBTI Cognitive Functions (LaTeX)\\n\\nYes, ultimately, the',\n",
       " 'Chapter Thirty Six\\nHomotopy Type Theory\\n\\nYes, ultimately, the',\n",
       " 'Chapter Thirty Six\\nCategory Theory For Goons\\n\\nYes, ultimately, the',\n",
       " 'Chapter Thirty Six\\nInvited To Leave The Group\\n\\nYes, ultimately, the',\n",
       " 'Chapter Thirty Six\\nESTJ Cognitive Functions Mathematics\\n\\nTo better depict',\n",
       " 'Chapter Thirty Six\\nISTJ Cognitive Functions Mathematics\\n\\nTo better depict',\n",
       " 'Chapter Thirty Six\\nENTJ Cognitive Functions Mathematics\\n\\nTo better depict',\n",
       " 'Chapter Thirty Six\\nINTJ Cognitive Functions Mathematics\\n\\nTo better depict',\n",
       " 'Chapter Thirty Six\\nESFJ Cognitive Functions Mathematics\\n\\nTo better depict',\n",
       " 'Chapter Thirty Six\\nISFJ Cognitive Functions Mathematics\\n\\nTo better depict',\n",
       " 'Chapter Thirty Six\\nENFJ Cognitive Functions Mathematics\\n\\nTo better depict',\n",
       " 'Chapter Thirty Six\\nINFJ Cognitive Functions Mathematics\\n\\nTo better depict',\n",
       " 'Chapter Thirty Six\\nESTP Cognitive Functions Mathematics\\n\\nTo better depict',\n",
       " 'Chapter Thirty Six\\nISTP Cognitive Functions Mathematics\\n\\nTo better depict',\n",
       " 'Chapter Thirty Six\\nENTP Cognitive Functions Mathematics\\n\\nTo better depict',\n",
       " 'Chapter Thirty Six\\nINTP Cognitive Functions Mathematics\\n\\nTo better depict',\n",
       " 'Chapter Thirty Six\\nESFP Cognitive Functions Mathematics\\n\\nTo better depict',\n",
       " 'Chapter Thirty Six\\nISFP Cognitive Functions Mathematics\\n\\nTo better depict',\n",
       " 'Chapter Thirty Six\\nENFP Cognitive Functions Mathematics\\n\\nTo better depict',\n",
       " 'Chapter Thirty Six\\nINFP Cognitive Functions Mathematics\\n\\nTo better depict',\n",
       " 'Chapter Thirty Six\\nEnneagram Topology\\n\\nTo better depict',\n",
       " 'Chapter Thirty Six\\nThe Meadow Mage\\n\\nTo better depict',\n",
       " 'Chapter Thirty Six\\nMarriage On Baker Street\\n\\nTo better depict',\n",
       " 'Chapter Thirty Six\\nSauron Is Good\\n\\nTo better depict',\n",
       " 'Chapter Thirty Six\\nHip Hop NLP\\n\\nTo better depict',\n",
       " 'Chapter Thirty Six\\nMayan Rituals\\n\\nTo better depict',\n",
       " 'Chapter Thirty Six\\nHonest Game Design\\n\\nTo better depict',\n",
       " 'Chapter Thirty Six\\nMicelia Yearnings\\n\\nTo better depict',\n",
       " 'Chapter Thirty Six\\nThe Functional Programming Curse\\n\\nTo better depict',\n",
       " \"Chapter Thirty Six\\nThe Magician's Apprentice\\n\\nTo better depict\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [\n",
    "\"\"\"Chapter Thirty Six\n",
    "Balenciaga\n",
    "\n",
    "Yes, ultimately, the\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "Recreative vs Fucked Up Drugs\n",
    "\n",
    "Yes, ultimately, the\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "Pink Floyd Mythology\n",
    "\n",
    "Yes, ultimately, the\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "What Myers-Briggs Couldn't Reach\n",
    "\n",
    "Yes, ultimately, the\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "Controversial Nipples\n",
    "\n",
    "Yes, ultimately, the\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "MBTI Cognitive Functions (LaTeX)\n",
    "\n",
    "Yes, ultimately, the\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "Homotopy Type Theory\n",
    "\n",
    "Yes, ultimately, the\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "Category Theory For Goons\n",
    "\n",
    "Yes, ultimately, the\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "Invited To Leave The Group\n",
    "\n",
    "Yes, ultimately, the\"\"\"\n",
    "] + list(map(lambda x: \"\"\"Chapter Thirty Six\n",
    "\"\"\" + x + \"\"\" Cognitive Functions Mathematics\n",
    "\n",
    "To better depict\"\"\", ['ESTJ', 'ISTJ', 'ENTJ', 'INTJ', 'ESFJ', 'ISFJ', 'ENFJ', 'INFJ',\n",
    "                      'ESTP', 'ISTP', 'ENTP', 'INTP', 'ESFP', 'ISFP', 'ENFP', 'INFP'])) + [\n",
    "\"\"\"Chapter Thirty Six\n",
    "Enneagram Topology\n",
    "\n",
    "To better depict\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "The Meadow Mage\n",
    "\n",
    "To better depict\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "Marriage On Baker Street\n",
    "\n",
    "To better depict\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "Sauron Is Good\n",
    "\n",
    "To better depict\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "Hip Hop NLP\n",
    "\n",
    "To better depict\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "Mayan Rituals\n",
    "\n",
    "To better depict\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "Honest Game Design\n",
    "\n",
    "To better depict\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "Micelia Yearnings\n",
    "\n",
    "To better depict\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "The Functional Programming Curse\n",
    "\n",
    "To better depict\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "The Magician's Apprentice\n",
    "\n",
    "To better depict\"\"\"\n",
    "]\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = [\n",
    "    {\n",
    "        'early_stopping': True,\n",
    "        'no_repeat_ngram_size': 2,\n",
    "        # 'num_beams': 5\n",
    "    },\n",
    "    # {\n",
    "    #     'early_stopping': True,\n",
    "    #     'no_repeat_ngram_size': 2,\n",
    "    #     # 'num_beams': 5,\n",
    "    #     'num_return_sequences': 5\n",
    "    # },\n",
    "    {\n",
    "        'do_sample': True,\n",
    "        'top_k': 0\n",
    "    },\n",
    "    {\n",
    "        'do_sample': True,\n",
    "        'top_k': 0,\n",
    "        'temperature': 0.7\n",
    "    },\n",
    "    {\n",
    "        'do_sample': True,\n",
    "        'top_k': 50\n",
    "    },\n",
    "    { # ##\n",
    "        'do_sample': True,\n",
    "        'no_repeat_ngram_size': 2,\n",
    "        'top_k': 50\n",
    "    },\n",
    "    {\n",
    "        'do_sample': True,\n",
    "        'no_repeat_ngram_size': 4,\n",
    "        'top_k': 50\n",
    "    },\n",
    "    {\n",
    "        'do_sample': True,\n",
    "        'no_repeat_ngram_size': 6,\n",
    "        'top_k': 50\n",
    "    },\n",
    "    {\n",
    "        'do_sample': True,\n",
    "        'top_k': 0,\n",
    "        'top_p': 0.92\n",
    "    },\n",
    "    {\n",
    "        'do_sample': True,\n",
    "        'temperature': 0.7,\n",
    "        'top_k': 50,\n",
    "        'top_p': 0.95\n",
    "    },\n",
    "    {\n",
    "        'do_sample': True,\n",
    "        'temperature': 0.6,\n",
    "        'top_k': 75,\n",
    "        'top_p': 0.88\n",
    "    },\n",
    "    {\n",
    "        'do_sample': True,\n",
    "        'temperature': 0.8,\n",
    "        'top_k': 25,\n",
    "        'no_repeat_ngram_size': 3,\n",
    "        'top_p': 0.7\n",
    "    },\n",
    "    {\n",
    "        'do_sample': True,\n",
    "        # 'num_return_sequences': 3,\n",
    "        'top_k': 50,\n",
    "        'top_p': 0.95\n",
    "    },\n",
    "    {\n",
    "        'early_stopping': True,\n",
    "        # 'num_beams': 5\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_file(path):\n",
    "    with open(path, 'r') as f: \n",
    "        body = json.load(f)\n",
    "        return body\n",
    "\n",
    "def write_file(path, dump):\n",
    "    with open(path, 'w') as f: \n",
    "        f.write(dump)\n",
    "\n",
    "def encode(text):\n",
    "    return tokenizer.encode(text, return_tensors='pt').to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import toml\n",
    "import re\n",
    "import json\n",
    "\n",
    "cache = read_file('./bloom-1b.json')\n",
    "\n",
    "def write_cache():\n",
    "    def read_row(row_input_raw, row_output):\n",
    "        rows = row_input_raw.split('\\n')\n",
    "        row_config = rows[0]\n",
    "        row_input = rows[1:]\n",
    "        return {\n",
    "            'config': rows[0],\n",
    "            'chapter': rows[1],\n",
    "            'title': rows[2],\n",
    "            'text': \"\\n\".join(rows[4:]) + row_output\n",
    "        }\n",
    "\n",
    "    rows = list(map(lambda row: read_row(row[0], row[1]), cache.items()))\n",
    "\n",
    "    rows.sort(key=lambda x: x['config'])\n",
    "    rows.sort(key=lambda x: x['title'])\n",
    "\n",
    "    def toc_row_link(title):\n",
    "        row_input = title.lower()\n",
    "        row_input = row_input.replace(' ', '-')\n",
    "        row_input = row_input.replace('.', '')\n",
    "        row_input = row_input.replace(',', '')\n",
    "        return row_input\n",
    "\n",
    "    def toc_row(title):\n",
    "        return f\"- [{title}](#{toc_row_link(title)})\"\n",
    "\n",
    "    rows_distinct = list(set(list(map(lambda row: row['title'], rows))))\n",
    "    rows_distinct.sort()\n",
    "\n",
    "    toc = \"\\n\".join(list(map(toc_row, rows_distinct)))\n",
    "\n",
    "    rows_md = list(map(lambda row: \"\\n\".join([\n",
    "        \"#### \" +  row['config'],\n",
    "        \"### \" + row['chapter'],\n",
    "        \"## \" + row['title'],\n",
    "        \"\",\n",
    "        row['text']\n",
    "    ]), rows))\n",
    "\n",
    "    config_link = re.sub(r'[{\",}]', '', config_dump.replace(': ', '='))\n",
    "\n",
    "    dump_md = toc + \"\\n\\n\\n\" + \"\\n\\n\\n\".join(rows_md)\n",
    "    write_file('./bloom-1b/' + config_link + '.md', dump_md)\n",
    "\n",
    "\n",
    "for input in inputs:\n",
    "    input_full = input_prefix + input\n",
    "    for config in config_list:\n",
    "        config_dump = json.dumps(config)\n",
    "        input_config = config_dump + \"\\n\" + input\n",
    "\n",
    "        try:\n",
    "            if input_config not in cache:\n",
    "                print(100 * '-')\n",
    "                print(input_config)\n",
    "                print(100 * '-')\n",
    "        \n",
    "                input_encoded = encode(input_full)\n",
    "                model_output = model.generate(\n",
    "                    input_encoded, \n",
    "                    max_length=2000, \n",
    "                    num_beams=config.get('num_beams', None), \n",
    "                    early_stopping=config.get('early_stopping', None),\n",
    "                    no_repeat_ngram_size=config.get('no_repeat_ngram_size', None),\n",
    "                    num_return_sequences=config.get('num_return_sequences', None),\n",
    "                    do_sample=config.get('do_sample', None),\n",
    "                    top_k=config.get('top_k', None),\n",
    "                    top_p=config.get('top_p', None),\n",
    "                    temperature=config.get('temperature', None),\n",
    "                )\n",
    "                output_raw = list(map(lambda x: tokenizer.decode(x, skip_special_tokens=True), model_output))\n",
    "                output = list(map(lambda x: x[len(input_full):], output_raw))\n",
    "                output_str = \"\\n\\n\\n\".join(output)\n",
    "                cache[input_config] = output_str\n",
    "\n",
    "                dump = json.dumps(cache, indent=2)\n",
    "                write_file('./bloom-1b.json', dump)\n",
    "\n",
    "                print(output_str)\n",
    "\n",
    "                write_cache()\n",
    "\n",
    "                time.sleep(2)\n",
    "                torch.cuda.empty_cache()\n",
    "                time.sleep(2)\n",
    "                \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e, input_config)\n",
    "            print(100 * '-')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "520e995520d0f28b9f1e7cacfd9ba1493aa60b57e5f0cc1543205df7dd9220a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
