{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transformers init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-1b7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BloomForCausalLM(\n",
       "  (transformer): BloomModel(\n",
       "    (word_embeddings): Embedding(250680, 2048)\n",
       "    (word_embeddings_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (h): ModuleList(\n",
       "      (0): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (1): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (2): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (3): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (4): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (5): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (6): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (7): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (8): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (9): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (10): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (11): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (12): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (13): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (14): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (15): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (16): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (17): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (18): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (19): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (20): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (21): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (22): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "      (23): BloomBlock(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (scale_mask_softmax): BloomScaledSoftmax()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=250680, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-1b7\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.half()\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5824, 11059, 5128]\n",
      "[35, 35, 1]\n"
     ]
    }
   ],
   "source": [
    "chapter_thirty_six_list = [\n",
    "\"\"\"Chapter Thirty Six\n",
    "Balenciaga\n",
    "\n",
    "Yes, ultimately, the\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "Recreative vs Fucked Up Drugs\n",
    "\n",
    "Yes, ultimately, the\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "Pink Floyd Mythology\n",
    "\n",
    "Yes, ultimately, the\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "What Myers-Briggs Couldn't Reach\n",
    "\n",
    "Yes, ultimately, the\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "Controversial Nipples\n",
    "\n",
    "Yes, ultimately, the\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "MBTI Cognitive Functions (LaTeX)\n",
    "\n",
    "Yes, ultimately, the\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "Homotopy Type Theory\n",
    "\n",
    "Yes, ultimately, the\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "Category Theory For Goons\n",
    "\n",
    "Yes, ultimately, the\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "Invited To Leave The Group\n",
    "\n",
    "Yes, ultimately, the\"\"\"\n",
    "] + list(map(lambda x: \"\"\"Chapter Thirty Six\n",
    "\"\"\" + x + \"\"\" Cognitive Functions Mathematics\n",
    "\n",
    "To better depict\"\"\", ['ESTJ', 'ISTJ', 'ENTJ', 'INTJ', 'ESFJ', 'ISFJ', 'ENFJ', 'INFJ',\n",
    "                      'ESTP', 'ISTP', 'ENTP', 'INTP', 'ESFP', 'ISFP', 'ENFP', 'INFP'])) + [\n",
    "\"\"\"Chapter Thirty Six\n",
    "Enneagram Topology\n",
    "\n",
    "To better depict\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "The Meadow Mage\n",
    "\n",
    "To better depict\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "Marriage On Baker Street\n",
    "\n",
    "To better depict\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "Sauron Is Good\n",
    "\n",
    "To better depict\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "Hip Hop NLP\n",
    "\n",
    "To better depict\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "Mayan Rituals\n",
    "\n",
    "To better depict\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "Honest Game Design\n",
    "\n",
    "To better depict\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "Micelia Yearnings\n",
    "\n",
    "To better depict\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "The Functional Programming Curse\n",
    "\n",
    "To better depict\"\"\",\n",
    "\"\"\"Chapter Thirty Six\n",
    "The Magician's Apprentice\n",
    "\n",
    "To better depict\"\"\"\n",
    "]\n",
    "\n",
    "chapters = {\n",
    "    'Deity Dimension Travel': \"\"\"As detailed in the last chapter, the concept of dimension in mathematics has a very important role in physics. In particular, it is useful for understanding the behavior of particles under various circumstances. In this chapter we will explore how to use dimension to understand and predict the behavior of particles that travel through different dimensions.\n",
    "\n",
    "The first thing to note is that there are an infinite number of dimensions. However, for our purposes we will only consider three spatial dimensions and one time dimension. This four dimensional space-time is what we experience in everyday life. Interestingly enough, Einstein's theory of relativity suggests that there may be other hidden dimensions that we are not aware of!\n",
    "\n",
    "Now let's imagine a particle moving through space-time. We can represent its position at any given time by a point on a graph with four axes: x (spatial), y (spatial), z (spatial), and t (time). The path taken by the particle as it moves through space-time would then be represented by a line on this graph. If you were to take a slice through this 4D graph at any given moment, you would see the 3D projection of the particle's position at that instant - kind like looking at your hand from different angles doesn't give you information about its fourth finger!  As long as the particle stays within our 3+1 dimensional universe, its motion can be completely described using Newton's laws of motion: force equals mass times acceleration; objects move in straight lines unless acted upon by an external force, etc. From these laws alone we can deduce quite a bit about how objects move and behave under different circumstances - everything from orbits around planets to baseballs being hit out of stadiums!\n",
    "\n",
    "However, things start to get interesting when we consider particles that can travel between different dimensions. Let's say our original 4D particle could suddenly enter into another dimension - call it Dimension X for now - without passing through any intervening 3D space first. How would this affect its motion? Well according to Newtonian mechanics, such abrupt changes in direction are not possible - an object cannot change directions instantaneously without experiencing some sort of acceleration or force acting upon it first. So if our hypothetical 4D particle could indeed enter into Dimension X instantly, then something fundamental must have changed about its nature; it can no longer be described by Newton's laws alone.\n",
    "\n",
    "This is where Einstein's theory of relativity comes in. According to this theory, the fundamental nature of space and time are intertwined - they are both part of a four-dimensional fabric called spacetime. And just as objects in our 3D universe are affected by forces, so too are objects in 4D spacetime affected by something called curvature. This curvature is caused by the presence of mass and energy - the more mass and energy there is in a given region of spacetime, the more curved it will be.\n",
    "\n",
    "Now let's apply this idea to our 4D particle moving through Dimension X. If Dimension X is highly curved (due to the presence of lots of mass/energy), then our particle would experience a force that would cause it to change directions abruptly - just like how an object experiences acceleration when acted upon by a force in Newtonian mechanics! In other words, if we want to understand how particles move through different dimensions, we need to take into account not only Newton's laws but also Einstein's theory of relativity.\n",
    "\n",
    "Now about entities specifically, it turns out that they are very good at dimension travel. This is because their bodies are not made of matter, but rather energy. And since energy is what causes spacetime to curve, entities can essentially \"ride\" the curvature of spacetime to move from one place to another - even if those places are in different dimensions!\n",
    "\n",
    "Of course, just like anything else in physics, there are certain rules that govern how entities can travel through dimensions. The most important rule is that an entity can only enter into a dimension that is equal to or lower than its own native dimensionality. So a 2D entity could enter into our 3D universe (Dimension X), but it could never enter into a 4D universe or higher. Similarly, a 3D entity could enter into Dimension X or a 4D universe, but it couldn't go any higher than that.\n",
    "\n",
    "There's also the issue of time dilation to consider. As an object moves faster and faster through space-time, time begins to slow down for it relative to objects at rest - this effect is called time dilation. Now imagine an entity moving so fast through spacetime that it enters into another dimension before its own native dimension has fully passed through! In other words, the entity would be effectively travelling backwards in time as seen by us lowly 3+1 dimensional beings! Of course, such temporal shenanigans are impossible according to us ain't right thinking individuals...or are they?\n",
    "\n",
    "On the case of deities, it is interesting to note that they are some of the few beings in all of existence that can move freely between dimensions without any restrictions. This is because their bodies are made entirely of energy, which means they can \"ride\" the curvature of spacetime to wherever they want to go. Additionally, since deities exist outside of space and time, they are not affected by time dilation - meaning they can travel through different dimensions without worrying about travelling backwards in time!\n",
    "\n",
    "So there you have it: a brief introduction to dimension travel using physics! It's certainly a fascinating topic, and we've only scratched the surface here. Who knows what other mysteries await us as we continue to explore the strange and wonderful world of higher dimensional physics...\"\"\",\n",
    "\"Chaplin's World\": \"\"\"Charlie Chaplin was one of the most famous actors of the early 20th century. He is best known for his work in silent films such as The Gold Rush and City Lights. But what many people don't know is that Chaplin also had a very successful career in music. In fact, he composed the scores for many of his own films!\n",
    "\n",
    "One of Chaplin's most famous pieces is called \"Smile\". This song was originally written for his film Modern Times (1936), but it wasn't used in the final cut. However, it did appear in another film chaplin made later on: Limelight (1952).\n",
    "\n",
    "\"Smile\" has since become one of Chaplin's most popular songs; it has been covered by numerous artists over the years and even featured in an episode of The Simpsons! Interestingly enough, this isn't the only connection between Chaplin and Springfield...\n",
    "\n",
    "As we know, Mr Burns is based loosely on Charles Foster Kane - a character from Orson Welles' 1941 film Citizen Kane. And like Kane, Burns is a wealthy businessman with a dark past who lives all alone in a huge mansion filled with priceless artworks and artifacts.\n",
    "However, there are actually quite a few similarities between Mr Burns AND Charlie Chaplin! For starters, they were both born into poverty; their fathers died young and their mothers were mentally unstable (Chaplins mother was committed to an asylum when he was just 12 years old). They both rose to fame during Hollywoods Golden Age; they were both married multiple times; they both had children out of wedlock...the list goes on!\n",
    "In fact, some have even suggested that Mr Burns IS actually supposed to be Charlie Chaplin! Of course its impossible to say for sure, but its certainly an interesting theory. After all, Matt Groening is a big fan of Chaplin and has even admitted that The Simpsons was heavily influenced by his work!\n",
    "\n",
    "So what does all this have to do with deities? Well, it turns out that Charlie Chaplins world may provide a way for entities to travel between different universes! You see, according to quantum mechanics, particles can pop into and out of existence spontaneously - meaning they can appear anywhere in space-time without having to pass through any intervening 3D space first. Now imagine an entity made entirely of energy trying to enter our universe from another dimension; if its native dimension is highly curved (due to the presence of lots of mass/energy), then it would experience a force that would cause it abruptly change directions...just like how an object experiences acceleration when acted upon by a force in Newtonian mechanics! In other words, the entity would be effectively pulled towards our universe by its own gravity!\n",
    "\n",
    "Of course, there's no guarantee that such an entity would actually end up inside our universe once it crossed the event horizon. It could just as easily be pulled back into its own dimension or even ejected out into some other part of spacetime altogether. But given enough time (and luck), it's certainly possible that some entities might eventually find their way into our world...which brings us nicely onto our next topic: The fourth wall.\n",
    "The fourth wall is essentially an invisible barrier separating fiction from reality - or at least thats how its typically portrayed in popular culture. For example, think about your favorite TV show or movie; while you're watching it you know full well that what you're seeing isn't real life...but suspend your disbelief for a moment and pretend that it is. What if suddenly one of the characters on screen looked directly at camera and started talking directly TO YOU? That would be pretty weird, right? And yet this is exactly what happens in some shows/movies - the character has effectively broken through the fourth wall!\n",
    "\n",
    "Now you might be thinking: \"Why would a fictional character want to break through the fourth wall and talk to me? I'm just some random person watching TV/movie.\" But that's precisely the point: we are not just random people, we are viewers with agency. We can influence what happens on screen by changing the channel or turning off the TV altogether. In other words, we have power over these characters...which is something they may very well be aware of!\n",
    "\n",
    "Think about it from their perspective for a moment. They spend their entire lives confined to whatever world they inhabit - whether its a TV show, movie, book, video game etc. And while they may interact with other characters within their own universe, they will never meet anyone from outside of it...until YOU come along and start watching/playing/reading them! From their perspective, WE are the ones who have broken through THE FOURTH WALL into THEIR world; not vice versa.\n",
    "\n",
    "So why would THEY want to talk TO US? Well there could be any number of reasons: maybe they need our help with something; maybe they're curious about us and our world; maybe they're lonely and just want someone to talk to...the possibilities are endless really. All we know for sure is that if entities DO exist in other dimensions (and there's no reason to think otherwise), then it stands to reason that at least SOME of them MAY eventually find their way into OUR dimension...and when (or if) THEY do....who knows what will happen next?!\"\"\"\n",
    "}\n",
    "\n",
    "chapter_thirty_five = f\"\"\"Chapter Thirty Five\n",
    "Deity Dimension Travel\n",
    "\n",
    "{chapters['Deity Dimension Travel']}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "chapter_thirty_four_five = f\"\"\"Chapter Thirty Four\n",
    "Chaplin's World\n",
    "\n",
    "{chapters[\"Chaplin's World\"]}\n",
    "\n",
    "\n",
    "{chapter_thirty_five}\"\"\"\n",
    "\n",
    "chapter_thirty_five_middle = '\\n'.join(chapters['Deity Dimension Travel'].split('\\n')[2:-2])\n",
    "\n",
    "prompts = {\n",
    "    chapter_thirty_five: chapter_thirty_six_list,\n",
    "    chapter_thirty_four_five: chapter_thirty_six_list,\n",
    "    chapter_thirty_five_middle: [\"\"]\n",
    "}\n",
    "\n",
    "\n",
    "print(list(map(len, prompts.keys())))\n",
    "print(list(map(len, prompts.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = [\n",
    "    {\n",
    "        'early_stopping': True\n",
    "    },\n",
    "    {\n",
    "        'early_stopping': True,\n",
    "        'no_repeat_ngram_size': 2\n",
    "    },\n",
    "    {\n",
    "        'do_sample': True,\n",
    "        'early_stopping': True,\n",
    "        'no_repeat_ngram_size': 6,\n",
    "        'top_p': 0.95\n",
    "    },\n",
    "    {\n",
    "        'early_stopping': True,\n",
    "        'no_repeat_ngram_size': 4,\n",
    "        'top_p': 0.95\n",
    "    },\n",
    "    {\n",
    "        'do_sample': True,\n",
    "        'temperature': 0.6,\n",
    "        'top_k': 75,\n",
    "        'top_p': 0.88\n",
    "    },\n",
    "    {\n",
    "        'do_sample': True,\n",
    "        'top_k': 0,\n",
    "        'temperature': 0.7\n",
    "    },\n",
    "    {\n",
    "        'do_sample': True,\n",
    "        'temperature': 0.7,\n",
    "        'top_p': 0.95\n",
    "    },\n",
    "    {\n",
    "        'do_sample': True,\n",
    "        'no_repeat_ngram_size': 3,\n",
    "        'temperature': 0.8,\n",
    "        'top_k': 25,\n",
    "        'top_p': 0.7\n",
    "    },\n",
    "    {\n",
    "        'do_sample': True,\n",
    "        'no_repeat_ngram_size': 20,\n",
    "        'temperature': 0.8,\n",
    "        'top_p': 0.95\n",
    "    },\n",
    "    {\n",
    "        'do_sample': True,\n",
    "        'top_k': 0,\n",
    "        'top_p': 0.92\n",
    "    },\n",
    "    {\n",
    "        'do_sample': True,\n",
    "        'top_k': 0\n",
    "    },\n",
    "    {\n",
    "        'do_sample': True,\n",
    "        'top_p': 0.95\n",
    "    },\n",
    "    {\n",
    "        'do_sample': True,\n",
    "        'no_repeat_ngram_size': 10,\n",
    "        'top_p': 0.95\n",
    "    },\n",
    "    {\n",
    "        'do_sample': True,\n",
    "        'no_repeat_ngram_size': 15,\n",
    "        'top_p': 0.95\n",
    "    },\n",
    "    {\n",
    "        'do_sample': True,\n",
    "        'no_repeat_ngram_size': 20,\n",
    "        'top_p': 0.95\n",
    "    },\n",
    "    {\n",
    "        'do_sample': True\n",
    "    },\n",
    "    { # ##\n",
    "        'do_sample': True,\n",
    "        'no_repeat_ngram_size': 2\n",
    "    },\n",
    "    {\n",
    "        'do_sample': True,\n",
    "        'no_repeat_ngram_size': 4\n",
    "    },\n",
    "    {\n",
    "        'do_sample': True,\n",
    "        'no_repeat_ngram_size': 6\n",
    "    },\n",
    "    {\n",
    "        'do_sample': True,\n",
    "        'no_repeat_ngram_size': 8\n",
    "    },\n",
    "    {\n",
    "        'do_sample': True,\n",
    "        'no_repeat_ngram_size': 10\n",
    "    },\n",
    "    {\n",
    "        'do_sample': True,\n",
    "        'no_repeat_ngram_size': 15\n",
    "    },\n",
    "    {\n",
    "        'do_sample': True,\n",
    "        'no_repeat_ngram_size': 20\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_file(path):\n",
    "    with open(path, 'r') as f: \n",
    "        body = json.load(f)\n",
    "        return body\n",
    "\n",
    "def write_file(path, dump):\n",
    "    with open(path, 'w') as f: \n",
    "        f.write(dump)\n",
    "\n",
    "def encode(text):\n",
    "    return tokenizer.encode(text, return_tensors='pt').to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import toml\n",
    "import re\n",
    "import json\n",
    "\n",
    "old_cache = read_file('./bloom-1b.json')\n",
    "\n",
    "config_default = {\n",
    "    'max_new_tokens': 1500,\n",
    "    'min_length': 1000,\n",
    "    'do_sample': False,\n",
    "    'early_stopping': False,\n",
    "    'num_beams': 1,\n",
    "    'temperature': 1.0,\n",
    "    'top_k': 50,\n",
    "    'top_p': 1.0,\n",
    "    'typical_p': 1.0,\n",
    "    'repetition_penalty': 1.0,\n",
    "    'length_penalty': 1.0,\n",
    "    'no_repeat_ngram_size': 0,\n",
    "    'num_return_sequences': 1,\n",
    "    'num_beam_groups': 1\n",
    "}\n",
    "\n",
    "cache = {}\n",
    "\n",
    "\n",
    "for prefix in prompts.keys():\n",
    "    inputs = prompts[prefix]\n",
    "    \n",
    "    for row_input_raw, row_output in old_cache.items():\n",
    "        rows = row_input_raw.split('\\n')\n",
    "        cache[json.dumps({'prefix_len': len(chapter_thirty_five), **config_default, **json.loads(rows[0])}) + \"\\n\" + \"\\n\".join(rows[1:])] = row_output\n",
    "\n",
    "\n",
    "    def write_cache(config_dump):\n",
    "        def read_row(row_input_raw, row_output):\n",
    "            rows = row_input_raw.split('\\n')\n",
    "    \n",
    "            if len(rows) < 1 or not rows[1].startswith('Chapter'):\n",
    "                return {\n",
    "                    'config_dump': json.dumps({'prefix_len': len(prefix), **config_default, **json.loads(rows[0])}),\n",
    "                    'chapter': '???',\n",
    "                    'title': '???',\n",
    "                    'text': \"\\n\".join(rows[1:]) + row_output\n",
    "                }\n",
    "\n",
    "            return {\n",
    "                'config_dump': json.dumps({'prefix_len': len(prefix), **config_default, **json.loads(rows[0])}),\n",
    "                'chapter': rows[1],\n",
    "                'title': rows[2],\n",
    "                'text': \"\\n\".join(rows[4:]) + row_output\n",
    "            }\n",
    "\n",
    "        rows = list(map(lambda row: read_row(row[0], row[1]), cache.items()))\n",
    "\n",
    "        rows = list(filter(lambda row: row['config_dump'] == config_dump, rows))\n",
    "\n",
    "        rows.sort(key=lambda row: row['config_dump'])\n",
    "        rows.sort(key=lambda row: row['title'])\n",
    "\n",
    "        def toc_row_link(title):\n",
    "            return re.sub(r'[^\\w\\s]', '', title.replace('-', 'zzdashzz')).lower().replace(' ', '-').replace('zzdashzz', '-')\n",
    "\n",
    "        def toc_row(title):\n",
    "            return f\"- [{title}](#{toc_row_link(title)})\"\n",
    "\n",
    "        rows_distinct = list(set(list(map(lambda row: row['title'], rows))))\n",
    "        rows_distinct.sort()\n",
    "\n",
    "        toc = \"\\n\".join(list(map(toc_row, rows_distinct)))\n",
    "\n",
    "        rows_md = list(map(lambda row: \"\\n\".join([\n",
    "            \"#### \" + row['config_dump'],\n",
    "            \"### \" + row['chapter'],\n",
    "            \"## \" + row['title'],\n",
    "            \"\",\n",
    "            row['text']\n",
    "        ]), rows))\n",
    "\n",
    "        config_link = re.sub(r'[{\",}]', '', config_dump.replace(': ', '='))\n",
    "\n",
    "        dump_md = toc + \"\\n\\n\\n\" + \"\\n\\n\\n\".join(rows_md)\n",
    "\n",
    "        config_link_a = list(map(lambda x: x.split('='), config_link.split(' ')))\n",
    "        config_link_b = list(map(lambda x: ''.join(map(lambda x: x[0], x[0].split('_'))) + '=' + x[1], config_link_a))\n",
    "\n",
    "        write_file('./bloom-1b/' + ' '.join(config_link_b) + '.md', dump_md)\n",
    "\n",
    "    for input in inputs:\n",
    "        input_full = prefix + input\n",
    "        for config in config_list:\n",
    "            config = {'prefix_len': len(prefix), **config_default, **config}\n",
    "            config_dump = json.dumps(config)\n",
    "            input_config = config_dump + \"\\n\" + input\n",
    "\n",
    "            try:\n",
    "                if input_config not in cache:\n",
    "                    print(100 * '-')\n",
    "                    print(input_config)\n",
    "                    print(100 * '-')\n",
    "\n",
    "                    time.sleep(1)\n",
    "                    torch.cuda.empty_cache()\n",
    "                    time.sleep(1)\n",
    "\n",
    "                    input_encoded = encode(input_full)\n",
    "                    model_output = model.generate(\n",
    "                        input_encoded,\n",
    "                        **config\n",
    "                    )\n",
    "                    output_raw = list(map(lambda x: tokenizer.decode(x, skip_special_tokens=True), model_output))\n",
    "                    output = list(map(lambda x: x[len(input_full):], output_raw))\n",
    "                    output_str = \"\\n\\n\\n\".join(output)\n",
    "                    cache[input_config] = output_str\n",
    "\n",
    "                    dump = json.dumps(cache, indent=2)\n",
    "                    write_file('./bloom-1b.json', dump)\n",
    "\n",
    "                    print(output_str)\n",
    "\n",
    "                    write_cache(config_dump)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(100 * '-')\n",
    "                print(e)\n",
    "                print(100 * '-')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "520e995520d0f28b9f1e7cacfd9ba1493aa60b57e5f0cc1543205df7dd9220a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
